# ğŸ“¡ Personal Downdetector

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=for-the-badge&logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit)
![SQLite](https://img.shields.io/badge/SQLite-003B57?style=for-the-badge&logo=sqlite)
![Status](https://img.shields.io/badge/Status-Finalizado-success?style=for-the-badge)

#### Projeto End-to-End de Engenharia de Dados
> Um monitor de disponibilidade de sites construÃ­do do zero para demonstrar o ciclo completo de dados: da coleta Ã  visualizaÃ§Ã£o.

__________________________

## ğŸ¯ Objetivo

Este projeto foi desenvolvido focando em **Engenharia de Dados**. O objetivo foi construir um **pipeline de dados** completo, funcional e desacoplado, que monitora a latÃªncia e disponibilidade de sites em tempo real.
O sistema verifica periodicamente uma lista de URLs, armazena mÃ©tricas de desempenho e disponibiliza um dashboard interativo para anÃ¡lise de incidentes.

__________________________

## ğŸ—ï¸ Arquitetura do Pipeline

O projeto segue uma arquitetura ETL (Extract, Transform, Load) simplificada, dividida em dois processos principais: o **Coletor (Backend)** e o **Visualizador (Frontend)**.

```mermaid
graph LR
    A[ğŸŒ Internet/Sites] -->|1. Coleta HTTP| B(ğŸ pipeline.py)
    B -->|2. IngestÃ£o & Tratamento| B
    B -->|3. PersistÃªncia| C[(ğŸ’¾ SQLite)]
    C -->|4. Leitura & TransformaÃ§Ã£o| D(ğŸ“Š dashboard.py)
    D -->|5. DisponibilizaÃ§Ã£o| E[ğŸ’» UsuÃ¡rio Final]
```

_________________________

## ğŸ”§ As 5 Etapas da Engenharia de Dados
> Conforme os requisitos do projeto, este pipeline contempla todas as etapas fundamentais:

#### 1. ğŸ“¶ Coleta (Collection)
**Fonte:** URLs externas (APIs e Websites).
- UtilizaÃ§Ã£o da biblioteca `requests` para enviar requisiÃ§Ãµes HTTP aos sites alvo.
- Captura do `status_code` (ex: 200, 404, 500) e cÃ¡lculo do tempo de resposta (latÃªncia).

#### 2. ğŸ”„ IngestÃ£o (Ingestion)
**Processo:** Script de automaÃ§Ã£o (`pipeline.py`).
- ImplementaÃ§Ã£o de um loop contÃ­nuo que executa a coleta a cada 60 segundos.
- Tratamento de erros de conexÃ£o (ex: site fora do ar ou sem internet) garantindo que o pipeline nÃ£o pare.

#### 3. ğŸ’¾ Armazenamento (Storage)
**Destino:** Banco de Dados Relacional.
- Os dados brutos sÃ£o persistidos em um banco SQLite local (`meu_monitor.db`).
- Estrutura da tabela logs: `id`, `url`, `status_code`, `tempo_resposta`, `data_hora`.

#### 4. âš™ï¸ TransformaÃ§Ã£o (Transformation)
**Processamento:** Limpeza e enriquecimento (`pandas` e `urllib`).
- **No Pipeline:** ConversÃ£o de segundos para milissegundos (ms).
- **No Dashboard:**
  - ConversÃ£o de strings de data para objetos `datetime` do Pandas.
  - Limpeza de URLs (extraÃ§Ã£o de domÃ­nios e remoÃ§Ã£o de "www") usando `urllib.parse` para melhor visualizaÃ§Ã£o.
  - RenomeaÃ§Ã£o de colunas para apresentaÃ§Ã£o (Snake Case -> Title Case).

#### 5. ğŸ“Š DisponibilizaÃ§Ã£o (Serving)
**Produto Final:** Dashboard Interativo.
- ConstruÃ­do com Streamlit.
- KPIs em Tempo Real: Cards com status (Online/Offline) e latÃªncia atual.
- AnÃ¡lise HistÃ³rica: GrÃ¡fico de linha interativo com filtro multiselect para comparar a performance dos sites ao longo do tempo.

_____________________________

## ğŸ’½ Como Rodar o Projeto
> Siga os passos abaixo para executar o monitor na sua mÃ¡quina local.

##### PrÃ©-requisitos
- Python 3.8 ou superior instalado.

### 1. InstalaÃ§Ã£o das DependÃªncias
> Abra o terminal na pasta do projeto e execute:

```Bash

pip install requests streamlit pandas schedule
```
-> **Nota:** O SQLite jÃ¡ vem nativo no Python, entÃ£o normalmente nÃ£o Ã© preciso instalar nada. Contudo, caso precisar, vocÃª pode instalar manualmente o pacote sqlite3:

```bash
pip install sqlite3
```

### 2. Executando o Pipeline (Backend)
> Este script ficarÃ¡ rodando em segundo plano, coletando dados a cada minuto.

```Bash

python pipeline.py
```
-> Deixe este terminal aberto.

### 3. Executando o Dashboard (Frontend)
> Abra um novo terminal (mantenha o anterior rodando) e execute:

```Bash

streamlit run dashboard.py
```
-> O navegador abrirÃ¡ automaticamente no endereÃ§o `http://localhost:8501`.

_____________________

## ğŸ“‚ Estrutura do Projeto

```Plaintext
personal-downdetector/
â”‚
â”œâ”€â”€ meu_monitor.db       # Banco de dados (gerado automaticamente)
â”œâ”€â”€ pipeline.py          # Script de Coleta e IngestÃ£o (Backend)
â”œâ”€â”€ dashboard.py         # AplicaÃ§Ã£o Streamlit (Frontend)
â””â”€â”€ README.md            # DocumentaÃ§Ã£o do projeto
```

_____________________

## ğŸ“¸ Screenshots

#### VisÃ£o Geral do Dashboard
<img width="1897" height="436" alt="Captura de tela 2025-12-10 174735" src="https://github.com/user-attachments/assets/cbe90ac6-b83a-4821-be7d-bb4f339fe93f" />

#### GrÃ¡fico de HistÃ³rico
<img width="1832" height="742" alt="Captura de tela 2025-12-10 174805" src="https://github.com/user-attachments/assets/ce6099b9-0573-4064-aa72-e3bec5f48e83" />

#### Tabela de HistÃ³rico 
<img width="1857" height="564" alt="Captura de tela 2025-12-10 174822" src="https://github.com/user-attachments/assets/76a6b3d6-1635-4c92-8021-79d0fe815913" />

_____________________

## ğŸ–¥ï¸ Tecnologias Utilizadas
- **Linguagem:** Python
- **OrquestraÃ§Ã£o:** Script Python (Loop/Time)
- **Banco de Dados:** SQLite3
- **Processamento de Dados:** Pandas
- **VisualizaÃ§Ã£o:** Streamlit

_________________________

<div align="center"> <sub>Projeto desenvolvido para fins educacionais por Rafael Bitu.</sub> </div>





















